#데이터 셋 load
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz

#압축 풀기
!tar -xf images.tar.gz
!tar -xf annotations.tar.gz

#필요 파일들
from tensorflow import keras
import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import load_img , img_to_array , array_to_img
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import os
import random

# 경로 만듦
input_dir = "images/"
target_dir = "annotations/trimaps/"

# 데이터 처리를 위한 준비(경로 리스트화)
input_img_paths =sorted([os.path.join(input_dir , fname )for fname in os.listdir(input_dir) if fname.endswith(".jpg")])
target_img_paths = sorted([os.path.join(target_dir , fname) for fname in os.listdir(target_dir) if fname.endswith(".png") and not fname.startswith(".")])

# Load Dataset and Preprocessing
print(len(input_img_paths))
print(len(target_img_paths))

img_siz=(160, 160) # size 줄임
n_class=3 # 클래스 3개
batch_size=48

# 이미지 뿌림
plt.axis("off")
plt.imshow(load_img(input_img_paths[10]))

# label 뿌림
plt.imshow(load_img(target_img_paths[10]))

# Ground truth에 대한 이미지를 화면에 표시하는 함수
def display_target(img):
  normalized_img = (img.astype("uint8") - 1) * 127 # (1, 2, 3을 0, 1, 2로 만듦) * 127 = 0, 127, 254로 만듦
  plt.axis("off")
  plt.imshow(normalized_img[: , : , 0])

img = load_img(target_img_paths[10], color_mode="grayscale")
img = img_to_array(img)
display_target(img)

class OxfordPets(keras.utils.Sequence): # 배치 단위로 순차적으로 들어감
    def __init__(self, batch_size,img_size,img_paths,label_paths): # 초기화 함수
        # data batch 단위로 구분해 가짐
        self.batch_size=batch_size
        self.img_size=img_size
        self.img_paths=img_paths
        self.label_paths=label_paths

    def __len__(self): # 전체 길이
        return len(self.img_paths)//self.batch_size # 나눗셈

    def __getitem__(self,idx): # 배치 단위 만큼 데이터 가져다 넣음
        i=idx*self.batch_size
        batch_img_paths=self.img_paths[i:i+self.batch_size] # 배치 더한 만큼 -> 한 번에 들어갈 양
        batch_label_paths=self.label_paths[i:i+self.batch_size]
        x=np.zeros((self.batch_size,)+self.img_size+(3,),dtype="float32")

        for j,path in enumerate(batch_img_paths):
            img=load_img(path,target_size=self.img_size) # img_size만큼 사용
            x[j]=img

        y=np.zeros((self.batch_size,)+self.img_size+(1,),dtype="uint8")
        for j,path in enumerate(batch_label_paths):
            img=load_img(path,target_size=self.img_size,color_mode="grayscale") # 2차원
            y[j]=np.expand_dims(img,2) # dimension 늘림
            y[j]-=1 # 부류 번호를 1, 2, 3에서 0, 1, 2로 변환
        return x,y # 배치 크기 만큼 img, label 생성 -> 데이터 만들어주는 과정

def conv2d_block(input_tensor, n_filters, kernel_size = 3):
    x = input_tensor
    for i in range(2):
        x = layers.SeparableConv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),
                                   depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', padding = 'same')(x) # padding 넣음
                                   # depthwise: 채널별로 conv, pointwise: 같은 위치에 해당하는 것들 다시 conv
        x = layers.Activation('relu')(x)
    return x    # 2번 적용

def encoder_block(inputs, n_filters=64, pool_size=(2, 2), dropout=0.3):
    f = conv2d_block(inputs, n_filters=n_filters)
    p = layers.MaxPooling2D(pool_size=(2, 2))(f)
    p = layers.Dropout(0.3)(p)
    return f, p   # p: maxpooling 적용

def encoder(inputs):
    # layer 4번
    f1, p1 = encoder_block(inputs, n_filters=64, pool_size=(2, 2), dropout=0.3)
    f2, p2 = encoder_block(p1, n_filters=128, pool_size=(2, 2), dropout=0.3)
    f3, p3 = encoder_block(p2, n_filters=256, pool_size=(2, 2), dropout=0.3)
    f4, p4 = encoder_block(p3, n_filters=512, pool_size=(2, 2), dropout=0.3)
    return p4, (f1, f2, f3, f4)   # feature map 꺼내진 상태, p4: 마지막 블록의 output, f: 각 conv이 저장된 결과

def bottleneck(inputs):
  bottle_neck = conv2d_block(inputs, n_filters=1024)
  # conv 2번, pooling 1번
  return bottle_neck

def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=3, dropout=0.3):
    u = layers.Conv2DTranspose(n_filters, kernel_size, strides=strides, padding='same')(inputs) # unpooling
    c = layers.concatenate([u, conv_output]) # feature map 연결
    c = layers.Dropout(dropout)(c)
    c = conv2d_block(c, n_filters, kernel_size=3) # conv 2번 적용
    return c

def decoder(inputs, convs, output_channels):
    f1, f2, f3, f4 = convs

    c6 = decoder_block(inputs, f4, n_filters=512, kernel_size=(3, 3), strides=(2, 2), dropout=0.3) # f4: 가장 밑단
    c7 = decoder_block(c6, f3, n_filters=256, kernel_size=(3, 3), strides=(2, 2), dropout=0.3)
    c8 = decoder_block(c7, f2, n_filters=128, kernel_size=(3, 3), strides=(2, 2), dropout=0.3)
    c9 = decoder_block(c8, f1, n_filters=64, kernel_size=(3, 3), strides=(2, 2), dropout=0.3)

    outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9) # output_channels: 최종 클래스 개수로 맞춤
    return outputs

# 모델 생성
def make_model(img_size, num_classes):
    inputs=keras.Input(shape=img_size+(3,))
    encoder_output, convs = encoder(inputs) # output 2개
    bottle_neck = bottleneck(encoder_output)
    outputs = decoder(bottle_neck, convs, 3)
    model = keras.Model(inputs=inputs, outputs=outputs)

    return model

model=make_model(img_siz, n_class) # 모델 생성 완료

random.Random(1337).shuffle(input_img_paths)
random.Random(1337).shuffle(target_img_paths)
test_samples=int(len(input_img_paths)*0.3)
train_img_paths=input_img_paths[:-test_samples]
train_label_paths=target_img_paths[:-test_samples]
test_img_paths=input_img_paths[-test_samples:]
test_label_paths=target_img_paths[-test_samples:]

# 배치 단위로 데이터 만듦
train_gen=OxfordPets(batch_size,img_siz,train_img_paths,train_label_paths) # 훈련 집합
test_gen=OxfordPets(batch_size,img_siz,test_img_paths,test_label_paths) # 검증 집합

# Training
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
cb=[keras.callbacks.ModelCheckpoint("oxford_segmentation.keras", save_best_only=True)] # save_best_only: 결과 좋아질 때만
history=model.fit(train_gen, epochs=10, validation_data=test_gen, callbacks=cb) # 모델 저장

preds=model.predict(test_gen)

# 결과 확인용
epochs = range(1, len(history.history["loss"]) + 1)
loss = history.history["loss"]
val_loss = history.history["val_loss"]
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()

# Visualization and Prediction
img1 = load_img(test_img_paths[0], target_size=(160, 160))
plt.axis("off")
plt.imshow(array_to_img(img1))

img = load_img(test_label_paths[0], color_mode="grayscale". target_size=(160, 160))
img = img_to_array(img)
display_target(img) # ground_truth: master data

mask = np.argmax(preds[0], axis=-1) # preds[0]: 영상 인덱스
mask *= 127 # (0, 1, 2)에 곱함
plt.axis("off")
plt.imshow(mask)

print("이미지 1 : ", mask.shape)
# 3개 채널로 구성: 3개 채널이 각 클래스에 대한 probability 담고 있음

img1 = load_img(test_img_paths[1], target_size=(160, 160))
plt.axis("off")
plt.imshow(array_to_img(img1))

img = load_img(test_label_paths[1], color_mode="grayscale". target_size=(160, 160))
img = img_to_array(img)
display_target(img) # ground_truth: master data
mask = np.argmax(preds[1], axis=-1) # preds[1]: 영상 인덱스
mask *= 127 # (0, 1, 2)에 곱함
plt.axis("off")
plt.imshow(mask)

print("이미지 1 : ", mask.shape)
